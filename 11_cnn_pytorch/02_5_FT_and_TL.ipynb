{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_5_FT_and_TL.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EP21104/EP21104.github.io/blob/main/11_cnn_pytorch/02_5_FT_and_TL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v9j_jX_yF4w"
      },
      "source": [
        "#ネットワークアーキテクチャ\n",
        " ネットワークの構造はあらかじめ人が決めておく必要があります．専門家でなければ，どのようなネットワーク構造が良いのか，１から決めるのは至難の技です．そこで，コンペティションなどで高い精度を達成しているネットワーク構造を利用します．\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R29Sq59byigJ"
      },
      "source": [
        "## AlexNet\n",
        "AlexNetは，2012年のILSVRCで優勝したモデルであり，畳み込み層が5層，全結合層が３層の８層構造です．当時のGPUのメモリ制約から，学習時には，各層の特徴マップをチャネル方向に分割し，2台のGPUで独立して学習するというアプローチが取られています．CNNの重みはガウス分布に従う乱数により初期化し，モーメンタム付きの確率的勾配降下法 (Stochastic Gradient Descent; SGD) により最適化します．学習時，誤差が低下しなくなったタイミングで学習率を1/10に減少させることも行われており，この最適化手法は，現在においてもベストプラクティスとして利用されています．AlexNetに導入された重要な要素技術として，Rectified Linear Units (ReLU)，Local Response Normalization (LRN)，Overlapping Pooling，Dropoutです．ReLUは，深いネットワークにおいてシグモイド関数などの従来の活性化関数を利用した場合に発生する勾配消失問題を解決することができる活性化関数です．LRNは，特徴マップの同一の位置にあり，隣接するチャネルの出力の値から，自身の出力の値を正規化する手法です．プーリング層は，sピクセルずつ離れたグリッドにおいて，周辺zピクセルの最大値や平均値で集約する処理を行います．通常はs = zとして重ならないようにしています．一方で，AlexNetでは，s = 2， z = 3として，集約されるピクセル領域がオーバラップするようにしています．このoverlapping poolingにより，過学習を低減し，わずかに最終的な精度が向上します．Dropoutは，学習時のネットワークについて，全結合層のユニットを一定確率で無効化する手法です．これにより，擬似的に毎回異なるアーキテクチャで学習を行うこととなり，アンサンブル学習と同様の効果が得られるため，より汎化されたモデルを学習することができます．\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/11_cnn_pytorch/02_5_finetuning/AlexNet.png?raw=true\" width = 90%>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eUc9hOsyksQ"
      },
      "source": [
        "## VGGNet\n",
        "VGGNetは，2014年のILSVRCにおいて，2位の認識精度を達成したモデルです．VGGNetは，シンプルな構造をしており，また学習済みモデルが公開されたことから，現在においてもベースラインのモデルとして利用されています．VGGNetは，1)フィルタサイズを3×3に統一する，2)同一フィルタ数の畳み込み層を幾つか積層した後に最大値プーリングを行い，特徴マップを半分に縮小する，3)最大値プーリング後の畳み込み層のフィルタ数を2倍に増加させる，というアイデアを導入しています． VGGNetには，いくつかのバリエーションがありますが，代表的な構造は13層の畳み込み層と3層の全結合層の全16層の構成です．VGGNetは，従来と比較して深いネットワークのため学習が難しいです．そのため，まず浅いネットワークを学習し，その後畳み込み層を追加した深いネットワークを学習する方法を取り入れています．\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/11_cnn_pytorch/02_5_finetuning/VGG.png?raw=true\" width = 70%>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWeNU-1Zym5_"
      },
      "source": [
        "## GoogLeNet\n",
        "GoogLeNetは，2014年のILSVRCの優勝モデルであり，Inception モジュールという複数の畳み込み層やpooling層から構成される小さなネットワークを9つ積層して全22層のネットワーク構造です．Inceptionモジュールは，ネットワークを分岐させてサイズの異なる畳み込みを行った後，それらの出力を連結する処理を行っています．また，GoogLeNetは，Global Average Pooling (GAP)を導入しています．従来のCNNモデルは，畳み込み層の後に複数の全結合層を重ねることで，最終的なクラス分類の出力を得る構造となっています．全結合層はパラメータ数が多く，また過学習を起こすことが課題となっており，dropoutを導入することで過学習を抑える必要がありました．一方，GAPは，入力された特徴マップのサイズと同じサイズのaverage poolingを行うプーリング層です．CNNの最後の畳み込み層の出力チャネル数を最終的な出力の次元数（クラス数）と同一とし，その後にGAPを適用することで，全結合層を利用することなく最終的な出力を得ることを提案しています．全結合層を利用しないことで，パラメータ数を大きく削減し，過学習を防ぐことができるようになりました．\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/11_cnn_pytorch/02_5_finetuning/GoogleNet.png?raw=true\" width = 100%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-3YwhuNyqNx"
      },
      "source": [
        "## ResNet\n",
        "Residual Networks (ResNet)は，2015年のILSVRCの優勝モデルです．VGGNetで示されたように，ネットワークを深くすることは表現能力を向上させ，認識精度を改善できます．しかし，あまりにも深いネットワークは効率的な学習が困難でした．ResNetは，通常のネットワークのように，何かしらの処理ブロックによる変換F(x)を単純に次の層に渡していくのではなく，その処理ブロックへの入力xをショートカットし， H(x) = F(x)+xを次の層に渡すようにしています．このショートカットを含めた処理単位をResidual Blockと呼びます．ResNetでは，ショートカットを通して，誤差逆伝播法時に勾配が直接下層に伝わっていくことになり，非常に深いネットワークにおいても効率的に学習ができるようになりました．Residual Blockは，3×3 のフィルタサイズを持つ畳み込み層とバッチ正規化，ReLUから構成されています．深いネットワークでは，ある層のパラメータの更新によって，その次の層への入力の分布がバッチ毎に大きく変化してまう内部共変量シフト (internal covariate shift) が発生し，学習が効率的に進まない問題がありました．バッチ正規化は，内部共変量シフトを正規化し，なるべく各レイヤが独立して学習が行えるようにすることで，学習を安定化・高速化する手法です．ResNetではこのバッチ正規化とショートカットをResidualモジュールに組み込むことで152層と非常に深いネットワークの学習を実現しています．\n",
        "\n",
        "ResNetが登場後，様々な派生系が提案され，代表的なネットワーク構造としては，WideResNet, PyramidNet, ResNeXt，Xception，DenseNetなどがあります．\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/11_cnn_pytorch/02_5_finetuning/ResidualBlock.png?raw=true\" width = 70%>\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/11_cnn_pytorch/02_5_finetuning/ResNet.png?raw=true\" width = 100%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBPwoZjNysSt"
      },
      "source": [
        "## アーキテクチャの自動探索\n",
        "人が最適なネットワーク構造を探し出すことはほぼ不可能です．そのため，学習により準最適なネットワーク構造の探索を行うことが注目されています．ネットワーク構造には，層数だけでなく，フィルタサイズやフィルタ数，また特徴マップの連結方法など，決めるべき対象が多く，全ての組み合わせを含めた最適化はコンピュータを利用したとしてもほぼ不可能です．そのため，ある程度設計したブロックをどのように組み合わせたら良いかを学習する手法が主流となっています．代表的な手法であるNeural Architecture Search (NAS) では，強化学習でネットワーク構造を探索しています．NASNetでは，探索する対象をResNetのResidual Blockのような塊（セル）をどのように組み合わせるかに焦点を当てています．MnasNetでは，モバイル端末などでも動作するようなネットワーク構造を探索します．NASやNASNetでは，認識精度を強化学習の報酬にしていましたが，MnasNetでは，処理速度も報酬に加えています．これにより，精度と速度を両立させたネットワーク構造を獲得できるようになりました．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7VTJKEfy4Su"
      },
      "source": [
        "## EfficientNet\n",
        "EfficientNetは，MnasNetにより得たネットワーク構造を拡張して精度を重視するネットワークに拡張しています．この時，各層のフィルタ数や層数，入力画像サイズの関係を定式化し，１つのパラメータでこれらの値を決定します．これにより，非常に簡単に新たな高精度なネットワーク構造を獲得できるようになりました．EfficientNetは，高精度度なだけでなく，様々なタスクのベースネットワークに活用され，転移学習に有用なネットワーク構造となっています．\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/11_cnn_pytorch/02_5_finetuning/EfficientNet.png?raw=true\" width = 100%>\n",
        "\n",
        "<img src=\"https://github.com/himidev/Lecture/blob/main/11_cnn_pytorch/02_5_finetuning/EfficientNetPerformance.png?raw=true\" width = 60%>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YMOjJfr4zrw"
      },
      "source": [
        "## 準備\n",
        "### Google Colaboratoryの設定確認・変更\n",
        "本チュートリアルではPyTorchを利用してニューラルネットワークの実装を確認，学習および評価を行います．\n",
        "**GPUを用いて処理を行うために，上部のメニューバーの「ランタイム」→「ランタイムのタイプを変更」からハードウェアアクセラレータをGPUにしてください．**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjcJxK6E4sYX"
      },
      "source": [
        "# ファインチューニングと転移学習\n",
        "\n",
        "深層学習の学習では，人が設計したネットワークモデルの初期パラメータを乱数で決めて学習を行います．\n",
        "別の手段として，大規模なデータセットで学習したパラメータを初期パラメータとして，学習を行う方法もあります．\n",
        "この時，学習には別の（小規模な）データセットを利用します．\n",
        "すなわち，小規模なデータセットで高い精度を達成するために，あらかじめ汎用的な特徴を大規模なデータセットで獲得しているネットワークを活用することになります．\n",
        "これを転移学習と言います．\n",
        "大規模なデータセットには，ImageNet(学習データ数100万枚)を利用することが多いです．\n",
        "深層学習の場合，利用するネットワークの出力層のみを学習することを転移学習，ネットワーク全体を学習することをファインチューニングと呼びます．ここでは，まず全体を学習するファインチューニングから説明します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-8Q_1cj4vwo"
      },
      "source": [
        "##ファインチューニング\n",
        "まず，学習を行う小規模なデータセットをダウンロードします．ここでは，Pytorchのチュートリアルでも利用しているハチとアリの２クラス識別を行うデータセットを利用します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R05NtjOH5B1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da4f7a2d-b5ea-4680-bf58-ddcc91c1c6c4"
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
        "!unzip -q hymenoptera_data.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-17 09:33:53--  https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 99.84.215.89, 99.84.215.59, 99.84.215.76, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|99.84.215.89|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 47286322 (45M) [application/zip]\n",
            "Saving to: ‘hymenoptera_data.zip’\n",
            "\n",
            "\rhymenoptera_data.zi   0%[                    ]       0  --.-KB/s               \rhymenoptera_data.zi 100%[===================>]  45.10M   294MB/s    in 0.2s    \n",
            "\n",
            "2025-11-17 09:33:53 (294 MB/s) - ‘hymenoptera_data.zip’ saved [47286322/47286322]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUE0hdmBR0PP"
      },
      "source": [
        "hymenoptera_dataというディレクトリの中にtrainとvalディレクトリがあります．その中にantsとbeesというディレクトリがあります．このようにクラスごとにデータを用意しておきます．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モジュールのインポート\n",
        "はじめに必要なモジュールをインポートします．\n",
        "\n",
        "### GPUの確認\n",
        "GPUを使用した計算が可能かどうかを確認します．\n",
        "\n",
        "`GPU availability: True`と表示されれば，GPUを使用した計算をPyTorchで行うことが可能です．\n",
        "Falseとなっている場合は，上記の「Google Colaboratoryの設定確認・変更」に記載している手順にしたがって，設定を変更した後に，モジュールのインポートから始めてください．"
      ],
      "metadata": {
        "id": "djgvrhvH5uRv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snt7BlEN43lI",
        "outputId": "fc98f67b-71f7-4c97-8c7e-f925c9466b09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# モジュールのインポート\n",
        "from time import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import torchsummary\n",
        "\n",
        "# GPUの確認\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print('Use CUDA:', use_cuda)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use CUDA: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70StG1_p5W_T"
      },
      "source": [
        "### データセットの読み込みと確認\n",
        "\n",
        "データ拡張の設定を行います．ここでは，ランダムリサイズクロップとランダムクリップをします．また，画像の正規化(明るさの正規化)も行います．\n",
        "データセットおよびデータ拡張をデータローダに与えます．\n",
        "hymenoptera_data/trainのデータをtrain_data, hymenoptera_data/valのデータをval_dataとします．datasets.ImageFolderを利用すると，指定したディレクトリ内にある各サブデイレクトリをクラスに対応してくれます．\n",
        "そして，クラス名はディレクトリ名から作成して学習データの場合はtrain_data.classesに含まれています．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBbG6vY35WCx",
        "outputId": "3991b5b6-bfa4-47fe-a5e7-4b4286c1a7d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_transforms = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "test_transforms = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "\n",
        "data_dir = 'hymenoptera_data'\n",
        "train_data = datasets.ImageFolder(\"hymenoptera_data/train\", train_transforms)\n",
        "val_data = datasets.ImageFolder(\"hymenoptera_data/val\", test_transforms)\n",
        "\n",
        "class_names = train_data.classes\n",
        "print(class_names)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ants', 'bees']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohdv3HZJTn5Y"
      },
      "source": [
        "学習済みモデルにはResNet18を利用します．pretrained = Trueにすると，ImageNetで学習したモデルを利用できます．ここで，ImageNetは1000クラスのデータセットです．すなわち，ImageNetで学習したResNet18の出力層のユニット数は1000になっています．ファインチューニングに利用するデータセットはハチとアリの２クラスなので，出力層のユニット数を変更します．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ubFUjQS6C6b",
        "outputId": "9870bffd-bf33-49ba-b231-3ddcfc23967e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "print(\"======== Original netowrk architecutre ========\\n\")\n",
        "print(model)\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n",
        "print(\"======== Fine-funing netowrk architecutre ========\\n\")\n",
        "print(model)\n",
        "\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# モデルの情報を表示\n",
        "torchsummary.summary(model, (3, 224, 224))\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 195MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Original netowrk architecutre ========\n",
            "\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n",
            "======== Fine-funing netowrk architecutre ========\n",
            "\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                    [-1, 2]           1,026\n",
            "================================================================\n",
            "Total params: 11,177,538\n",
            "Trainable params: 11,177,538\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 42.64\n",
            "Estimated Total Size (MB): 106.00\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoMgEiJhVDVW"
      },
      "source": [
        "### 学習\n",
        "１回の誤差を算出するデータ数（ミニバッチサイズ）を16，学習エポック数を25とします．\n",
        "hymenoptera_dataの学習データ数を取得し，１エポック内における更新回数を求めます．\n",
        "学習モデルに`image`を与えて各クラスの確率yを取得します．各クラスの確率yと教師ラベル`label`との誤差をsoftmax coross entropy誤差関数で算出します．\n",
        "また，認識精度も算出します．そして，誤差をbackward関数で逆伝播し，ネットワークの更新を行います．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAqishQQ6WKW"
      },
      "source": [
        "# ミニバッチサイズ・エポック数の設定\n",
        "batch_size = 16\n",
        "epoch_num = 25\n",
        "n_iter = len(train_data) / batch_size\n",
        "\n",
        "# データローダーの設定\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# 誤差関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if use_cuda:\n",
        "    criterion.cuda()\n",
        "\n",
        "# ネットワークを学習モードへ変更\n",
        "model.train()\n",
        "\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    sum_loss = 0.0\n",
        "    count = 0\n",
        "\n",
        "    for image, label in train_loader:\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        y = model(image)\n",
        "        loss = criterion(y, label)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        sum_loss += loss.item()\n",
        "\n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "    print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed_time :{}\".format(epoch,\n",
        "                                                                                 sum_loss / n_iter,\n",
        "                                                                                 count.item() / len(train_data),\n",
        "                                                                                 time() - start))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HRTxLRKYt51"
      },
      "source": [
        "### テスト\n",
        "学習したネットワークモデルを用いて評価を行います．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データローダーの準備\n",
        "test_loader = torch.utils.data.DataLoader(val_data, batch_size=100, shuffle=False)\n",
        "\n",
        "# ネットワークを評価モードへ変更\n",
        "model.eval()\n",
        "\n",
        "# 評価の実行\n",
        "count = 0\n",
        "with torch.no_grad():\n",
        "    for image, label in test_loader:\n",
        "\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        y = model(image)\n",
        "\n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "print(\"test accuracy: {}\".format(count.item() / len(val_data)))"
      ],
      "metadata": {
        "id": "15uhvPfR7rUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "認識結果を確認します．"
      ],
      "metadata": {
        "id": "0D2MO67q85kV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXtayVmJ6cjJ"
      },
      "source": [
        "def tensor_to_numpy(inp):\n",
        "  \"imshow for Tensor\"\n",
        "  inp = inp.numpy().transpose((1,2,0))\n",
        "  mean = np.array([0.485, 0.456, 0.406])\n",
        "  std = np.array([0.229, 0.224, 0.225])\n",
        "  inp = std * inp + mean\n",
        "  inp = np.clip(inp, 0, 1)\n",
        "  return inp\n",
        "\n",
        "model.eval()\n",
        "num_images = 4\n",
        "count = 0\n",
        "fig = plt.figure()\n",
        "\n",
        "# データローダーの準備\n",
        "test_loader = torch.utils.data.DataLoader(val_data, batch_size=num_images, shuffle=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (image, label) in enumerate(test_loader):\n",
        "        image = image.cuda()\n",
        "        label = label.cuda()\n",
        "\n",
        "        y = model(image)\n",
        "        _, preds = torch.max(y, 1)\n",
        "\n",
        "        for j in range(image.size()[0]):\n",
        "            count += 1\n",
        "            ax = fig.add_subplot(num_images//2, 2, count)\n",
        "            ax.axis('off')\n",
        "            ax.set_title('predicted: {}  label: {}'\n",
        "                         .format(class_names[preds[j]], class_names[label[j]]))\n",
        "            ax.imshow(tensor_to_numpy(image.cpu().data[j]))\n",
        "\n",
        "        if count >= num_images:\n",
        "            break\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Grad-CAMによるAttention mapの可視化\n",
        "Grad-CAMを利用するために必要なツールをインストールします．\n",
        "Grad-CAMは，`pytorch-gradcam`というツールをインストールすることで簡単に利用することができます．\n"
      ],
      "metadata": {
        "id": "O1mQUZHp9617"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aojl-RiO3xoW"
      },
      "source": [
        "!pip install pytorch-gradcam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grad-CAMによりAttention mapを可視化して，ネットワークの判断根拠を確認してみます． ここでは，hymenoptera_dataのフォルダにある画像を直接指定して可視化させます．可視化には，には，Pytorchのtorchvisionに用意されている用意されているmake_gridを利用します．"
      ],
      "metadata": {
        "id": "O3Vk62CSAZIo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2Hv095F377z"
      },
      "source": [
        "from gradcam.utils import visualize_cam\n",
        "from gradcam import GradCAM\n",
        "from torchvision.utils import make_grid\n",
        "from PIL import Image\n",
        "\n",
        "# Grad-CAM\n",
        "target_layer = model.layer4 # ex., layer1, layer2, layer3, layer3[1], layer4[0]\n",
        "gradcam = GradCAM(model, target_layer)\n",
        "\n",
        "img = Image.open(\"/content/hymenoptera_data/val/ants/1053149811_f62a3410d3.jpg\")\n",
        "\n",
        "torch_img = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])(img)\n",
        "\n",
        "if use_cuda:\n",
        "    torch_img = torch_img.cuda()\n",
        "\n",
        "normed_input_img = transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])(torch_img)[None]\n",
        "mask, _ = gradcam(normed_input_img)\n",
        "heatmap, result = visualize_cam(mask, torch_img)\n",
        "\n",
        "grid_image = make_grid([torch_img.cpu(), result], nrow=5)\n",
        "\n",
        "transforms.ToPILImage()(grid_image)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1ldePYCZBUR"
      },
      "source": [
        "##転移学習\n",
        "ネットワークの変更した層のみを学習させて転移学習します．\n",
        "その場合，各層に勾配を逆伝播しないよう，requires_grad を Falseにします．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained=True)\n",
        "print(\"======== Original netowrk architecutre ========\\n\")\n",
        "print(model)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n",
        "print(\"======== Fine-funing netowrk architecutre ========\\n\")\n",
        "print(model)\n",
        "\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# モデルの情報を表示\n",
        "torchsummary.summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "disucgsBBBmF",
        "outputId": "68c0cf18-7100-4927-c63b-8521604e8c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'models' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1767383242.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"======== Original netowrk architecutre ========\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習\n",
        "１回の誤差を算出するデータ数（ミニバッチサイズ）を16，学習エポック数を25とします．\n",
        "hymenoptera_dataの学習データ数を取得し，１エポック内における更新回数を求めます．\n",
        "学習モデルに`image`を与えて各クラスの確率yを取得します．各クラスの確率yと教師ラベル`label`との誤差をsoftmax coross entropy誤差関数で算出します．\n",
        "また，認識精度も算出します．そして，誤差をbackward関数で逆伝播し，ネットワークの更新を行います．\n"
      ],
      "metadata": {
        "id": "pw8O8fFHBWVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ミニバッチサイズ・エポック数の設定\n",
        "batch_size = 16\n",
        "epoch_num = 25\n",
        "n_iter = len(train_data) / batch_size\n",
        "\n",
        "# データローダーの設定\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# 誤差関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if use_cuda:\n",
        "    criterion.cuda()\n",
        "\n",
        "# ネットワークを学習モードへ変更\n",
        "model.train()\n",
        "\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    sum_loss = 0.0\n",
        "    count = 0\n",
        "\n",
        "    for image, label in train_loader:\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        y = model(image)\n",
        "        loss = criterion(y, label)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        sum_loss += loss.item()\n",
        "\n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "    print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed_time :{}\".format(epoch,\n",
        "                                                                                 sum_loss / n_iter,\n",
        "                                                                                 count.item() / len(train_data),\n",
        "                                                                                 time() - start))\n"
      ],
      "metadata": {
        "id": "K06n5d0kBM7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テスト\n",
        "学習したネットワークモデルを用いて評価を行います．"
      ],
      "metadata": {
        "id": "g-4PjoEIBklE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw3UZeGvPVl9"
      },
      "source": [
        "# データローダーの準備\n",
        "test_loader = torch.utils.data.DataLoader(val_data, batch_size=100, shuffle=False)\n",
        "\n",
        "# ネットワークを評価モードへ変更\n",
        "model.eval()\n",
        "\n",
        "# 評価の実行\n",
        "count = 0\n",
        "with torch.no_grad():\n",
        "    for image, label in test_loader:\n",
        "\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        y = model(image)\n",
        "\n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "print(\"test accuracy: {}\".format(count.item() / len(val_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a41HYYwVZsyR"
      },
      "source": [
        "##EfficientNetのファインチューニング\n",
        "ResNetとともに高い精度を達成しているネットワークモデルであるEfficientNetを学習済みモデルとして利用します．EfficientNetをpipでインストールします．EfficientNetはB0からB7までネットワーク構造が違うモデルがあります．ここではB1を利用します．学習済みモデルを読み込む際，転移学習後のクラス数を指定することができます．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet_pytorch\n"
      ],
      "metadata": {
        "id": "KM9ZgvsrDQbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jHh5Kb3LvG3"
      },
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "model = EfficientNet.from_pretrained('efficientnet-b1', num_classes = 2)\n",
        "print(\"======== Fine-funing netowrk architecutre ========\\n\")\n",
        "print(model)\n",
        "\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# モデルの情報を表示\n",
        "torchsummary.summary(model, (3, 224, 224))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習\n",
        "１回の誤差を算出するデータ数（ミニバッチサイズ）を16，学習エポック数を25とします．\n",
        "hymenoptera_dataの学習データ数を取得し，１エポック内における更新回数を求めます．\n",
        "学習モデルに`image`を与えて各クラスの確率yを取得します．各クラスの確率yと教師ラベル`label`との誤差をsoftmax coross entropy誤差関数で算出します．\n",
        "また，認識精度も算出します．そして，誤差をbackward関数で逆伝播し，ネットワークの更新を行います．\n"
      ],
      "metadata": {
        "id": "2Q5-34xLCwKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ミニバッチサイズ・エポック数の設定\n",
        "batch_size = 16\n",
        "epoch_num = 25\n",
        "n_iter = len(train_data) / batch_size\n",
        "\n",
        "# データローダーの設定\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# 誤差関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if use_cuda:\n",
        "    criterion.cuda()\n",
        "\n",
        "# ネットワークを学習モードへ変更\n",
        "model.train()\n",
        "\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    sum_loss = 0.0\n",
        "    count = 0\n",
        "\n",
        "    for image, label in train_loader:\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        y = model(image)\n",
        "        loss = criterion(y, label)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        sum_loss += loss.item()\n",
        "\n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "    print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed_time :{}\".format(epoch,\n",
        "                                                                                 sum_loss / n_iter,\n",
        "                                                                                 count.item() / len(train_data),\n",
        "                                                                                 time() - start))\n"
      ],
      "metadata": {
        "id": "8NyYtB_yCs9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テスト\n",
        "学習したネットワークモデルを用いて評価を行います．"
      ],
      "metadata": {
        "id": "O-YUVTu5C5Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データローダーの準備\n",
        "test_loader = torch.utils.data.DataLoader(val_data, batch_size=100, shuffle=False)\n",
        "\n",
        "# ネットワークを評価モードへ変更\n",
        "model.eval()\n",
        "\n",
        "# 評価の実行\n",
        "count = 0\n",
        "with torch.no_grad():\n",
        "    for image, label in test_loader:\n",
        "\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        y = model(image)\n",
        "\n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "print(\"test accuracy: {}\".format(count.item() / len(val_data)))"
      ],
      "metadata": {
        "id": "Co7R-N87C0yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlgSO_rA8zBo"
      },
      "source": [
        "#課題\n",
        "1. EfficientNetをB1からそれ以外の構造に変えて精度の変化を比較しましょう．\n",
        " \"efficientnet-b1\" を　\"efficientnet-b0\"，\"efficientnet-b2\"，，，\"efficientnet-b7\"のいずれかにすれば良いです．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J870mOYG9Q-t",
        "outputId": "2676499c-41c7-4ab7-824f-6a1fd0e38f5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#ここにコードを書く\n",
        "\n",
        "#ヒント: AlexNet(学習済みモデル)を２クラス識別にするためには以下のようにします．\n",
        "model = models.alexnet(pretrained=True)\n",
        "num_ftrs = model.classifier[6].in_features\n",
        "model.classifier[6] = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#学習\n",
        "# ミニバッチサイズ・エポック数の設定\n",
        "batch_size = 16\n",
        "epoch_num = 25\n",
        "n_iter = len(train_data) / batch_size\n",
        "\n",
        "# データローダーの設定\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# 誤差関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if use_cuda:\n",
        "    criterion.cuda()\n",
        "\n",
        "# ネットワークを学習モードへ変更\n",
        "model.train()\n",
        "\n",
        "\n",
        "start = time()\n",
        "for epoch in range(1, epoch_num+1):\n",
        "    sum_loss = 0.0\n",
        "    count = 0\n",
        "\n",
        "    for image, label in train_loader:\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        y = model(image)\n",
        "        loss = criterion(y, label)\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        sum_loss += loss.item()\n",
        "\n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "    print(\"epoch: {}, mean loss: {}, mean accuracy: {}, elapsed_time :{}\".format(epoch,\n",
        "                                                                                 sum_loss / n_iter,\n",
        "                                                                                 count.item() / len(train_data),\n",
        "                                                                                 time() - start))\n",
        "\n",
        "#テスト\n",
        "# データローダーの準備\n",
        "test_loader = torch.utils.data.DataLoader(val_data, batch_size=100, shuffle=False)\n",
        "\n",
        "# ネットワークを評価モードへ変更\n",
        "model.eval()\n",
        "\n",
        "# 評価の実行\n",
        "count = 0\n",
        "with torch.no_grad():\n",
        "    for image, label in test_loader:\n",
        "\n",
        "        if use_cuda:\n",
        "            image = image.cuda()\n",
        "            label = label.cuda()\n",
        "\n",
        "        y = model(image)\n",
        "\n",
        "        pred = torch.argmax(y, dim=1)\n",
        "        count += torch.sum(pred == label)\n",
        "\n",
        "print(\"test accuracy: {}\".format(count.item() / len(val_data)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'models' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4091282343.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#ヒント: AlexNet(学習済みモデル)を２クラス識別にするためには以下のようにします．\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malexnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnum_ftrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ftrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUXrGBwf9wT0"
      },
      "source": [
        "2. AlexNetで学習済みモデルを利用する場合としない場合での精度を比較しましょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GclP-4NG94Od"
      },
      "source": [
        "#ヒント: AlexNet(学習済みモデル)を２クラス識別にするためには以下のようにします．\n",
        "model = models.alexnet(pretrained=True)\n",
        "num_ftrs = model.classifier[6].in_features\n",
        "model.classifier[6] = nn.Linear(num_ftrs, 2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}